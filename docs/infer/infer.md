#

## `infer`

This command requests an inference with specified tokens, `states`, `transformers`, `sampler` and `terminal`. Read other sections of the doc if you have no idea about them yet.

## Example

#### Request

```jsonc
{
    "echo_id": ...,
    "command": "infer",

    "data": {
            // Tokens used by each state in the inference pipeline.
            // The token order must match the order of state.
            // A prompt can be either a string, or a list of integers
            // and/or strings.
            "tokens": ["prompt1", "prompt2", ...],
            // State IDs for all states used in the pipeline. Note that
            // auto-regressive generation will use the same token for 
            // all states in the pipeline.
            "states": ["state1", "state2", ...],
            // Transformer IDs used by each state in the inference
            // pipeline. The order of IDs must match the oder of state.
            // Transformers can be chained, so a list of transformers
            // can be specified to process logits in sequence.
            "transformers": [[...], [...], ...],
            // The sampler used by the pipeline. Only 1 token will be
            // sampled and used in subsequent generation, so only 1
            // sampler can be specified.
            "sampler": "sampler_id",
            // The normalizer used by the pipeline. Normalizer is used
            // to give out a token probs distribution from one or more
            // logits distributions. The field can be omitted to use
            // softmax on first logits.
            "normalizer": "normalizer_id",
            // The terminal ID specified to control the terminal
            // condition of this pipeline.
            "terminal": "terminal_id",
            // Should the prompt fed into the pipeline update the
            // transformers/sampler? This only affects to the prompt
            // input, tokens generated in auto-regressive loop will
            // always update the pipeline.
            "update_prompt": false,
            // Should the pipeline resets transformers/sampler when
            // any of them is exhausted?
            "reset_on_exhaustion": true,
            // Should the pipeline updates the state(s) when infer is
            // finished and state is unloaded/synced?
            "update_states": true | [true, false, ...]
    }
}
```

#### Response

```jsonc
{
    "echo_id": ...,
    "status": "success",
    "duration_ms": ...,

    "result": {
        // The returned valid UTF8-string decoded from tokens.
        "value": ...,
        // The last token generated by current inference, using this
        // token as the prompt with a same pipeline can continue the
        // generation.
        "last_token": ...,
        // The count of tokens inferred in this request.
        "inferred_tokens": ...,
        // The reason of why the inference is ended, can either be:
        // - `by_terminal`: ended due to terminal is triggered.
        // - `by_exhaustion`: ended due to transformer/sampler is exhausted.
        "end_reason": "by_exhaustion"
    }
}
```
