#

## `infer`

This command requests an inference with specified tokens, `states`, `transformers`, `sampler` and `terminal`. Read other sections of the doc if you have no idea about them yet.

## Example

#### Request

```jsonc
{
    "echo_id": ...,
    "command": "infer",
    "data": {
            // Tokens used by each state in the inference pipeline.
            // The token order must match the order of state.
            // A prompt can be either a string, or a list of integers
            // and/or strings.
            "tokens": ["prompt1", "prompt2", ...],
            // State IDs for all states used in the pipeline. Note that
            // auto-regressive generation will use the same token for 
            // all states in the pipeline.
            "states": ["state1", "state2", ...],
            // Pipeline ID to be used in the infer loop. The pipeline
            // will be updated as the infer goes.
            "pipeline": "pipeline_id",
            // Should the prompt fed into the pipeline update the
            // transformers/sampler? This only affects to the prompt
            // input, tokens generated in auto-regressive loop will
            // always update the pipeline.
            "update_prompt": false,
            // Should the pipeline resets transformers/sampler when
            // any of them is exhausted?
            "reset_on_exhaustion": true,
            // How long before the infer request returns with a
            // timeout error in ms? By default it's 20s.
            "timeout": 20000
    }
}
```

#### Response

```jsonc
{
    "echo_id": ...,
    "status": "success",
    "duration_ms": ...,

    "result": {
        // The returned valid UTF8-string decoded from tokens.
        "result": ...,
        // The last token generated by current inference, using this
        // token as the prompt with a same pipeline can continue the
        // generation.
        "last_token": ...,
        // The count of tokens inferred in this request.
        "inferred_tokens": ...,
        // The count of tokens encoded from the prompt.
        "prompt_tokens": ...,
        // The reason of why the inference is ended, can either be:
        // - `by_terminal`: ended due to terminal is triggered.
        // - `by_exhaustion`: ended due to transformer/sampler is exhausted.
        // - `by_eof`: ended due to token `0` is generated. You should
        // insert token/prompt instead of `0` to continue generation.
        // - `by_max_token`: ended due to the hard limit is reached 
        // (as configured)
        "end_reason": "by_exhaustion"
    }
}
```
