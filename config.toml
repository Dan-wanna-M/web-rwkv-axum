# Config for web-rwkv-axum
# It specifies what model to use, what token table to use, etc.

[axum]

[generation]

[model]
# Path to the model file
# Must be a safetensor instead of pth.
path = "assets/RWKV-4-World-7B-v1-20230626-ctx4096.st"
# Max batch count means maximum size of model state used
# as pool for batch inference. Larger size reduce the need
# of buffer swapping at the cost of increased GPU memory
# usage. Default 32.
max_batch_count = 8
# Max concurrency means the maximum concurrent tasks that
# can be inferred at the same time. Larger size reduces
# response performance but increase GPU utilization to a
# level. Default 8.
max_concurrency = 8
# Max chunk count means how many tokens should a chunk
# be at max. Larger chunks increase infer speed for long
# prompt at the cost of resource consumption.
max_chunk_count = 256
# Preference for adapter. Can be HighPerformance or
# LowPower. If omitted, adapter index will be used.
preference = "HighPerformance"
# Adapter index to select, usually 0.
# There might be a tool to check for adapters later.
# adapter = 0
# Quantize a certain layer of the model, a bit flag.
# If omitted, no quantization is done.
# quantization = 4

[tokenizer]
# Path to the vocab JSON.
# Refer to https://github.com/cryscan/web-rwkv/blob/main/assets/rwkv_vocab_v20230424.json
path = "assets/rwkv_vocab_v20230424.json"
